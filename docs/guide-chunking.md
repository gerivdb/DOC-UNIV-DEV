# üîß Guide Complet du Chunking pour RAG
## Strat√©gies Optimales par Type de Contenu

### üéØ **Objectif du Guide**

Ce guide fournit des **strat√©gies pratiques et optimis√©es** pour le chunking de diff√©rents types de documents dans le contexte d'un syst√®me RAG (Retrieval-Augmented Generation), avec un focus sur la **qualit√© de la r√©cup√©ration** et la **coh√©rence s√©mantique**.

---

## üìã **Principes Fondamentaux du Chunking**

### **1. √âquilibre Taille vs Contexte**
- **Chunks trop petits** : Perte de contexte, information fragment√©e
- **Chunks trop grands** : Dilution de l'information pertinente, embedding moins pr√©cis
- **Zone optimale** : 200-1000 tokens selon le type de contenu

### **2. Pr√©servation de la Coh√©rence S√©mantique**
- Respecter les unit√©s logiques naturelles (paragraphes, sections, fonctions)
- √âviter de couper au milieu des phrases ou concepts
- Maintenir le contexte n√©cessaire √† la compr√©hension

### **3. Strat√©gies d'Overlap**
- **Overlap fixe** : 10-20% pour maintenir la continuit√©
- **Overlap s√©mantique** : Bas√© sur la similarit√© du contenu
- **Overlap contextuel** : Pr√©servation des r√©f√©rences et liens

---

## üìÑ **CHUNKING DOCUMENTS PDF**

### **Strat√©gie G√©n√©rale PDF**

```python
# Configuration optimale pour PDF
PDF_CHUNKING_CONFIG = {
    "chunk_size": 800,           # Tokens
    "overlap": 150,              # Tokens (18.75%)
    "min_chunk_size": 200,       # √âviter chunks trop petits
    "respect_paragraphs": True,   # Ne pas couper mid-paragraph
    "preserve_structure": True    # Maintenir headers/sections
}
```

### **PDF Acad√©miques (Papers, Th√®ses)**

**Caract√©ristiques** : Structure formelle, r√©f√©rences, figures, tableaux

```yaml
academic_pdf_strategy:
  chunk_by: "section"           # Respecter la structure acad√©mique
  chunk_size: 1000             # Plus grande pour contexte acad√©mique
  overlap: 200                 # Overlap plus important
  preserve_elements:
    - headers                  # Titres de sections
    - citations               # R√©f√©rences bibliographiques  
    - figure_captions         # L√©gendes de figures
    - table_content          # Contenu des tableaux
  special_handling:
    abstract: "single_chunk"   # Abstract toujours dans un seul chunk
    conclusion: "single_chunk" # Conclusion pr√©serv√©e
    references: "separate"     # Bibliographie s√©par√©e
```

**Exemple d'impl√©mentation** :

```python
def chunk_academic_pdf(pdf_text, metadata):
    chunks = []
    
    # 1. Extraction des sections
    sections = extract_sections(pdf_text)
    
    for section in sections:
        if section.type == "abstract":
            # Abstract toujours en un seul chunk
            chunks.append({
                "content": section.content,
                "type": "abstract",
                "section": section.title,
                "metadata": metadata
            })
        elif section.type == "content":
            # Chunking normal avec respect des paragraphes
            section_chunks = semantic_split(
                section.content, 
                max_size=1000,
                overlap=200,
                respect_boundaries=True
            )
            for i, chunk in enumerate(section_chunks):
                chunks.append({
                    "content": chunk,
                    "type": "content",
                    "section": section.title,
                    "chunk_index": i,
                    "metadata": metadata
                })
    
    return chunks
```

### **PDF Techniques (Manuels, Documentation)**

**Caract√©ristiques** : Code snippets, diagrammes, proc√©dures

```yaml
technical_pdf_strategy:
  chunk_size: 600              # Plus petit pour pr√©cision technique
  overlap: 100
  preserve_structure: True
  special_elements:
    code_blocks:
      strategy: "complete"      # Ne jamais couper le code
      max_size: 1500           # Taille max pour code blocks
    procedures:
      strategy: "step_by_step"  # Respecter les √©tapes
    diagrams:
      strategy: "with_caption"  # Inclure l√©gende + description
```

---

## üíª **CHUNKING CODE SOURCE**

### **Strat√©gie G√©n√©rale Code**

```python
CODE_CHUNKING_CONFIG = {
    "chunk_by": "function",      # Unit√© logique naturelle
    "max_chunk_size": 1200,      # Plus grand pour fonctions complexes
    "min_chunk_size": 100,       # √âviter snippets trop petits
    "include_context": True,     # Inclure imports/classes parentes
    "preserve_completeness": True # Fonctions compl√®tes
}
```

### **Python / JavaScript / TypeScript**

**Strat√©gies par √©l√©ment** :

```yaml
python_chunking:
  functions:
    strategy: "complete_function"    # Fonction compl√®te avec docstring
    max_size: 1500
    include_decorators: True
    include_imports: "relevant"      # Seulement imports utilis√©s
  
  classes:
    strategy: "method_by_method"     # Chaque m√©thode = chunk
    include_class_context: True     # Docstring classe + __init__
    preserve_inheritance: True
  
  modules:
    strategy: "logical_blocks"       # Groupes de fonctions li√©es
    respect_comments: True          # Utiliser comments comme s√©parateurs
```

**Exemple d'impl√©mentation** :

```python
def chunk_python_code(code_content, file_path):
    import ast
    
    chunks = []
    tree = ast.parse(code_content)
    
    # Extraction des imports
    imports = get_imports(tree)
    
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            # Fonction compl√®te avec contexte
            function_code = extract_function_with_context(
                node, code_content, imports
            )
            
            chunks.append({
                "content": function_code,
                "type": "function",
                "name": node.name,
                "file_path": file_path,
                "start_line": node.lineno,
                "docstring": ast.get_docstring(node),
                "complexity": calculate_complexity(node)
            })
        
        elif isinstance(node, ast.ClassDef):
            # Classe : chunk par m√©thode
            class_chunks = chunk_class_methods(node, code_content, imports)
            chunks.extend(class_chunks)
    
    return chunks
```

### **SQL / Configurations**

```yaml
sql_chunking:
  queries:
    strategy: "complete_statement"   # Query compl√®te
    max_size: 800
    include_comments: True
  
  schemas:
    strategy: "table_by_table"       # Une table par chunk
    include_relationships: True      # Foreign keys contexte
  
  procedures:
    strategy: "complete_procedure"   # Proc√©dure compl√®te
    max_size: 2000
```

---

## üìù **CHUNKING MARKDOWN**

### **Strat√©gie G√©n√©rale Markdown**

```python
MARKDOWN_CHUNKING_CONFIG = {
    "chunk_by": "section",           # Headers comme d√©limiteurs naturels
    "chunk_size": 600,               # Optimal pour contenu textuel
    "overlap": 100,
    "preserve_hierarchy": True,      # Maintenir H1 > H2 > H3
    "include_headers": True          # Headers dans chaque chunk
}
```

### **Documentation Technique**

**Caract√©ristiques** : Headers structur√©s, code blocks, liens

```yaml
tech_markdown_strategy:
  hierarchy_respect: True
  sections:
    h1: "preserve_complete"          # Sections compl√®tes si possible
    h2: "split_if_large"            # Subdiviser si > 1000 tokens
    h3: "keep_together"             # Garder sous-sections ensemble
  
  special_elements:
    code_blocks:
      strategy: "preserve_complete"  # Code jamais coup√©
      include_language: True         # Pr√©server language hints
    
    tables:
      strategy: "complete_table"     # Table jamais coup√©e
      max_size: 1500                # Limite pour grandes tables
    
    lists:
      strategy: "logical_groups"     # Grouper items li√©s
      max_items_per_chunk: 10
```

**Exemple d'impl√©mentation** :

```python
def chunk_markdown(md_content, metadata):
    import markdown
    from markdown.treeprocessors import Treeprocessor
    
    chunks = []
    
    # Parse markdown structure
    md = markdown.Markdown(extensions=['toc'])
    html = md.convert(md_content)
    toc = md.toc_tokens
    
    current_chunk = ""
    current_header = ""
    
    for line in md_content.split('\n'):
        if line.startswith('#'):
            # Nouveau header : finaliser chunk pr√©c√©dent
            if current_chunk and len(current_chunk.split()) > 50:
                chunks.append({
                    "content": current_chunk.strip(),
                    "header": current_header,
                    "type": "section",
                    "metadata": metadata
                })
            
            # Commencer nouveau chunk
            current_header = line.strip()
            current_chunk = line + '\n'
        else:
            current_chunk += line + '\n'
            
            # V√©rifier taille chunk
            if len(current_chunk.split()) > 600:
                # Chercher point de coupure naturel
                split_point = find_natural_split(current_chunk)
                if split_point:
                    chunks.append({
                        "content": current_chunk[:split_point],
                        "header": current_header,
                        "type": "section_part",
                        "metadata": metadata
                    })
                    current_chunk = current_chunk[split_point:]
    
    # Dernier chunk
    if current_chunk.strip():
        chunks.append({
            "content": current_chunk.strip(),
            "header": current_header,
            "type": "section",
            "metadata": metadata
        })
    
    return chunks
```

### **Articles de Blog / Content Marketing**

```yaml
blog_markdown_strategy:
  chunk_size: 400                    # Plus petit pour contenu web
  overlap: 80
  respect_elements:
    - paragraphs                     # Paragraphes complets
    - quotes                         # Citations pr√©serv√©es
    - image_captions                 # L√©gendes avec images
  
  seo_preservation:
    include_title: True              # Titre dans chaque chunk
    preserve_keywords: True          # Maintenir mots-cl√©s importants
```

---

## üß† **CHUNKING S√âMANTIQUE AVANC√â**

### **Segmentation par Similarit√©**

```python
def semantic_chunking(text, embedding_model, similarity_threshold=0.8):
    """
    Chunking bas√© sur la similarit√© s√©mantique entre phrases
    """
    sentences = split_into_sentences(text)
    embeddings = [embedding_model.encode(s) for s in sentences]
    
    chunks = []
    current_chunk = [sentences[0]]
    
    for i in range(1, len(sentences)):
        # Calculer similarit√© avec chunk actuel
        chunk_embedding = mean(embeddings[:i])
        sentence_embedding = embeddings[i]
        similarity = cosine_similarity(chunk_embedding, sentence_embedding)
        
        if similarity >= similarity_threshold:
            current_chunk.append(sentences[i])
        else:
            # Nouvelle section s√©mantique : finaliser chunk
            chunks.append(' '.join(current_chunk))
            current_chunk = [sentences[i]]
    
    # Dernier chunk
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks
```

### **Chunking Hi√©rarchique**

```python
def hierarchical_chunking(document, levels=["section", "paragraph", "sentence"]):
    """
    Chunking multi-niveau avec pr√©servation hi√©rarchie
    """
    hierarchy = {}
    
    # Level 1: Sections
    sections = split_by_sections(document)
    for i, section in enumerate(sections):
        section_id = f"sec_{i}"
        hierarchy[section_id] = {
            "content": section,
            "level": "section",
            "children": {}
        }
        
        # Level 2: Paragraphes
        paragraphs = split_by_paragraphs(section)
        for j, paragraph in enumerate(paragraphs):
            para_id = f"para_{i}_{j}"
            hierarchy[section_id]["children"][para_id] = {
                "content": paragraph,
                "level": "paragraph",
                "parent": section_id,
                "children": {}
            }
            
            # Level 3: Phrases (si n√©cessaire)
            if len(paragraph.split()) > 200:
                sentences = split_by_sentences(paragraph)
                for k, sentence in enumerate(sentences):
                    sent_id = f"sent_{i}_{j}_{k}"
                    hierarchy[section_id]["children"][para_id]["children"][sent_id] = {
                        "content": sentence,
                        "level": "sentence",
                        "parent": para_id
                    }
    
    return hierarchy
```

---

## ‚ö° **OPTIMISATIONS DE PERFORMANCE**

### **Chunking Parall√®le**

```python
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

def parallel_chunking(documents, chunking_strategy, max_workers=None):
    """
    Chunking parall√®le pour traitement de gros volumes
    """
    if max_workers is None:
        max_workers = multiprocessing.cpu_count()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for doc in documents:
            future = executor.submit(chunking_strategy, doc)
            futures.append(future)
        
        results = []
        for future in futures:
            try:
                chunks = future.result(timeout=60)  # 1 min timeout
                results.extend(chunks)
            except Exception as e:
                print(f"Erreur chunking: {e}")
        
        return results
```

### **Cache Intelligent**

```python
class ChunkingCache:
    def __init__(self, cache_size=1000):
        self.cache = {}
        self.cache_size = cache_size
        self.access_order = []
    
    def get_chunks(self, content_hash, chunking_params):
        cache_key = (content_hash, str(chunking_params))
        
        if cache_key in self.cache:
            # Move to end (most recently used)
            self.access_order.remove(cache_key)
            self.access_order.append(cache_key)
            return self.cache[cache_key]
        
        return None
    
    def store_chunks(self, content_hash, chunking_params, chunks):
        cache_key = (content_hash, str(chunking_params))
        
        # √âviction LRU si cache plein
        if len(self.cache) >= self.cache_size:
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]
        
        self.cache[cache_key] = chunks
        self.access_order.append(cache_key)
```

---

## üìä **√âVALUATION QUALIT√â CHUNKING**

### **M√©triques d'√âvaluation**

```python
def evaluate_chunking_quality(chunks, ground_truth=None):
    """
    √âvaluation de la qualit√© du chunking
    """
    metrics = {}
    
    # 1. Coh√©rence de taille
    sizes = [len(chunk["content"].split()) for chunk in chunks]
    metrics["size_consistency"] = {
        "mean": np.mean(sizes),
        "std": np.std(sizes),
        "coefficient_variation": np.std(sizes) / np.mean(sizes)
    }
    
    # 2. Couverture du contenu
    total_content = " ".join([chunk["content"] for chunk in chunks])
    original_content = ground_truth if ground_truth else total_content
    metrics["content_coverage"] = len(total_content) / len(original_content)
    
    # 3. Coh√©rence s√©mantique interne
    if len(chunks) > 1:
        coherence_scores = []
        for chunk in chunks:
            sentences = split_into_sentences(chunk["content"])
            if len(sentences) > 1:
                # Coh√©rence interne du chunk
                sentence_embeddings = [embed_sentence(s) for s in sentences]
                coherence = calculate_coherence(sentence_embeddings)
                coherence_scores.append(coherence)
        
        metrics["semantic_coherence"] = np.mean(coherence_scores)
    
    # 4. Qualit√© de l'overlap
    if len(chunks) > 1:
        overlap_quality = []
        for i in range(len(chunks) - 1):
            current = chunks[i]["content"]
            next_chunk = chunks[i + 1]["content"]
            overlap_score = calculate_overlap_quality(current, next_chunk)
            overlap_quality.append(overlap_score)
        
        metrics["overlap_quality"] = np.mean(overlap_quality)
    
    return metrics
```

### **Tests de R√©gression**

```python
def chunking_regression_test(test_cases, chunking_function):
    """
    Tests de non-r√©gression pour chunking
    """
    results = {}
    
    for test_name, test_data in test_cases.items():
        input_text = test_data["input"]
        expected_chunks = test_data["expected_chunks"]
        
        # Chunking actuel
        actual_chunks = chunking_function(input_text)
        
        # Comparaison
        results[test_name] = {
            "passed": compare_chunks(expected_chunks, actual_chunks),
            "expected_count": len(expected_chunks),
            "actual_count": len(actual_chunks),
            "quality_score": evaluate_chunking_quality(actual_chunks)
        }
    
    return results
```

---

## üéØ **RECOMMANDATIONS PAR CAS D'USAGE**

### **Documentation Technique**
- **Taille** : 600-800 tokens
- **Overlap** : 15-20%
- **Strat√©gie** : Section-based + code preservation
- **M√©trique cl√©** : Completeness + Contextual coherence

### **Code Source**
- **Taille** : 800-1200 tokens  
- **Overlap** : 10-15%
- **Strat√©gie** : Function/class-based
- **M√©trique cl√©** : Syntactic completeness + Logical unity

### **Contenu Acad√©mique**
- **Taille** : 800-1000 tokens
- **Overlap** : 20-25%
- **Strat√©gie** : Section-based + citation preservation
- **M√©trique cl√©** : Semantic coherence + Reference integrity

### **Articles Web**
- **Taille** : 400-600 tokens
- **Overlap** : 15-20%
- **Strat√©gie** : Paragraph-based + keyword preservation
- **M√©trique cl√©** : Readability + SEO value retention

---

## üõ†Ô∏è **OUTILS ET IMPL√âMENTATION**

### **Librairies Recommand√©es**

```bash
# Installation des d√©pendances
pip install langchain-text-splitters
pip install semantic-text-splitter
pip install nltk spacy
pip install sentence-transformers
pip install tiktoken  # Pour tokenization OpenAI
```

### **Pipeline de Production**

```python
class ProductionChunkingPipeline:
    def __init__(self, config):
        self.config = config
        self.cache = ChunkingCache()
        self.metrics_collector = MetricsCollector()
    
    def process_document(self, document, doc_type):
        # 1. D√©tection du type si non sp√©cifi√©
        if not doc_type:
            doc_type = self.detect_document_type(document)
        
        # 2. S√©lection strat√©gie
        strategy = self.get_strategy_for_type(doc_type)
        
        # 3. Cache check
        content_hash = self.hash_content(document.content)
        cached_chunks = self.cache.get_chunks(content_hash, strategy.params)
        if cached_chunks:
            return cached_chunks
        
        # 4. Chunking
        chunks = strategy.chunk(document)
        
        # 5. Post-processing
        chunks = self.post_process_chunks(chunks, doc_type)
        
        # 6. Quality check
        quality_score = self.evaluate_quality(chunks)
        if quality_score < self.config.min_quality_threshold:
            # Retry avec strat√©gie alternative
            chunks = self.fallback_chunking(document, doc_type)
        
        # 7. Cache storage
        self.cache.store_chunks(content_hash, strategy.params, chunks)
        
        # 8. Metrics collection
        self.metrics_collector.record_chunking(doc_type, quality_score, len(chunks))
        
        return chunks
```

---

## üìà **MONITORING ET AM√âLIORATION CONTINUE**

### **M√©triques de Production**

```yaml
chunking_monitoring:
  performance_metrics:
    - "chunking_latency_p95"      # 95e percentile latence
    - "chunks_per_document_avg"   # Nombre moyen chunks/doc
    - "chunk_size_distribution"   # Distribution tailles
    - "quality_score_avg"         # Score qualit√© moyen
  
  quality_metrics:
    - "semantic_coherence_avg"    # Coh√©rence s√©mantique
    - "content_coverage_ratio"    # Couverture contenu
    - "overlap_quality_score"     # Qualit√© overlap
    
  business_metrics:
    - "retrieval_accuracy"        # Pr√©cision r√©cup√©ration
    - "user_satisfaction_score"   # Satisfaction utilisateurs
    - "false_positive_rate"       # Taux faux positifs
```

### **Optimisation Continue**

```python
class ChunkingOptimizer:
    def __init__(self):
        self.experiments = {}
        self.baseline_metrics = {}
    
    def run_ab_test(self, strategy_a, strategy_b, test_documents):
        """
        A/B test entre deux strat√©gies de chunking
        """
        results_a = self.evaluate_strategy(strategy_a, test_documents)
        results_b = self.evaluate_strategy(strategy_b, test_documents)
        
        # Statistical significance test
        p_value = self.statistical_test(results_a, results_b)
        
        return {
            "strategy_a": results_a,
            "strategy_b": results_b,
            "winner": "A" if results_a["score"] > results_b["score"] else "B",
            "confidence": 1 - p_value,
            "significant": p_value < 0.05
        }
```

---

**Status** : ‚úÖ **PRODUCTION READY**  
**Version** : 1.0.0  
**Derni√®re mise √† jour** : 2025-11-03

Ce guide constitue la **r√©f√©rence compl√®te** pour l'impl√©mentation de strat√©gies de chunking optimales dans l'√©cosyst√®me DOC-UNIV-DEV, garantissant une qualit√© maximale de r√©cup√©ration et de coh√©rence s√©mantique.